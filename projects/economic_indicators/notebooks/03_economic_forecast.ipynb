{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c131b6a-dcaf-45ec-b7ba-6db048677529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPLETE ECONOMIC FORECASTING IMPLEMENTATION\n",
    "# Step-by-step: Data Processing ‚Üí Modeling ‚Üí Evaluation\n",
    "# ============================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82397ebf-c2c0-4e25-9aa5-0070196e7bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7a458fa-c4f0-479a-a15e-e9e2ba237707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical models\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbc4d281-ff6a-47c0-ae4f-5dabafcfc080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet\n",
    "from prophet import Prophet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7cc764f-6ad0-48d9-9b95-f82214eb5dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80f19eae-f10a-4eb9-8573-4008cb2e26c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 1: HANDLE MISSING VALUES\n",
    "# ============================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb8a373b-1a81-406d-8972-9c8405eab6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df, method='forward_fill'):\n",
    "    \"\"\"\n",
    "    Handle missing values in time series data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Time series data with datetime index\n",
    "    method : str\n",
    "        'forward_fill', 'interpolate', or 'hybrid'\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Data with missing values handled\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 1: HANDLING MISSING VALUES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_before = df.isnull().sum()\n",
    "    missing_pct = (missing_before / len(df)) * 100\n",
    "    \n",
    "    print(\"\\nüìä Missing Values Before:\")\n",
    "    for col in df.columns:\n",
    "        if missing_before[col] > 0:\n",
    "            print(f\"   {col}: {missing_before[col]} ({missing_pct[col]:.2f}%)\")\n",
    "    \n",
    "    if missing_before.sum() == 0:\n",
    "        print(\"   ‚úÖ No missing values found!\")\n",
    "        return df\n",
    "    \n",
    "    # Apply selected method\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f69aad07-e7b6-4a78-8000-3f0c9c56f2eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 7)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mFile \u001b[39m\u001b[32m<string>:7\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31melif method == 'interpolate':\u001b[39m\n                                 ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    " if method == 'forward_fill':\n",
    "        print(\"\\nüîß Applying forward fill...\")\n",
    "        df_clean = df_clean.fillna(method='ffill')\n",
    "        # Backward fill any remaining NaN at start\n",
    "        df_clean = df_clean.fillna(method='bfill')\n",
    "        \n",
    "    elif method == 'interpolate':\n",
    "        print(\"\\nüîß Applying linear interpolation...\")\n",
    "        df_clean = df_clean.interpolate(method='linear', limit_direction='both')\n",
    "        \n",
    "    elif method == 'hybrid':\n",
    "        print(\"\\nüîß Applying hybrid approach...\")\n",
    "        # Interpolate for small gaps (<5 values)\n",
    "        df_clean = df_clean.interpolate(method='linear', limit=5)\n",
    "        # Forward fill for larger gaps\n",
    "        df_clean = df_clean.fillna(method='ffill')\n",
    "        df_clean = df_clean.fillna(method='bfill')\n",
    "    \n",
    "    # Verify\n",
    "    missing_after = df_clean.isnull().sum()\n",
    "    print(\"\\nüìä Missing Values After:\")\n",
    "    if missing_after.sum() == 0:\n",
    "        print(\"   ‚úÖ All missing values handled!\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è Remaining missing values: {missing_after.sum()}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "136a1f86-73e0-48d7-84d2-29eb199c72c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 2: TEST STATIONARITY & APPLY DIFFERENCING\n",
    "# ============================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b85d7b88-1d16-4b19-9cac-99cbb4ea3973",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_stationarity(series, name='Series'):\n",
    "    \"\"\"\n",
    "    Perform Augmented Dickey-Fuller test for stationarity\n",
    "    \"\"\"\n",
    "    result = adfuller(series.dropna())\n",
    "    \n",
    "    print(f\"\\nüìä ADF Test Results for {name}:\")\n",
    "    print(f\"   ADF Statistic: {result[0]:.6f}\")\n",
    "    print(f\"   p-value: {result[1]:.6f}\")\n",
    "    print(f\"   Critical Values:\")\n",
    "    for key, value in result[4].items():\n",
    "        print(f\"      {key}: {value:.3f}\")\n",
    "    \n",
    "    if result[1] < 0.05:\n",
    "        print(f\"   ‚úÖ STATIONARY (p < 0.05)\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"   ‚ùå NON-STATIONARY (p >= 0.05)\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae0fc8ba-1822-4173-ade7-61c936854331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_stationary(df, columns=None):\n",
    "    \"\"\"\n",
    "    Apply differencing to non-stationary series\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Original time series data\n",
    "    columns : list\n",
    "        Columns to check and difference (None = all)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (df_stationary, difference_orders)\n",
    "        Stationary data and dict of difference orders applied\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 2: TESTING STATIONARITY & DIFFERENCING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if columns is None:\n",
    "        columns = df.columns\n",
    "    \n",
    "    df_stationary = df.copy()\n",
    "    difference_orders = {}\n",
    "    \n",
    "    for col in columns:\n",
    "        print(f\"\\nüî¨ Testing: {col}\")\n",
    "        \n",
    "        series = df[col].dropna()\n",
    "        is_stationary = test_stationarity(series, col)\n",
    "        \n",
    "        if is_stationary:\n",
    "            difference_orders[col] = 0\n",
    "            continue\n",
    "        \n",
    "        # Try first difference\n",
    "        print(f\"\\n   Applying first difference...\")\n",
    "        diff1 = series.diff().dropna()\n",
    "        is_stationary_diff1 = test_stationarity(diff1, f\"{col} (1st diff)\")\n",
    "        \n",
    "        if is_stationary_diff1:\n",
    "            df_stationary[col] = df[col].diff()\n",
    "            difference_orders[col] = 1\n",
    "        else:\n",
    "            # Try second difference\n",
    "            print(f\"\\n   Applying second difference...\")\n",
    "            diff2 = diff1.diff().dropna()\n",
    "            is_stationary_diff2 = test_stationarity(diff2, f\"{col} (2nd diff)\")\n",
    "            \n",
    "            if is_stationary_diff2:\n",
    "                df_stationary[col] = df[col].diff().diff()\n",
    "                difference_orders[col] = 2\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è Still non-stationary after 2nd difference\")\n",
    "                df_stationary[col] = df[col].diff().diff()\n",
    "                difference_orders[col] = 2\n",
    "    \n",
    "    # Remove NaN created by differencing\n",
    "    df_stationary = df_stationary.dropna()\n",
    "    \n",
    "    print(\"\\nüìä Summary:\")\n",
    "    for col, order in difference_orders.items():\n",
    "        print(f\"   {col}: {'No differencing' if order == 0 else f'{order} difference(s)'}\")\n",
    "    \n",
    "    return df_stationary, difference_orders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cab816d4-d44d-48a8-be30-7e31c529d8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 3: FEATURE ENGINEERING\n",
    "# ============================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9d15886-4029-43b9-ab01-0fb1b7f67e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, target_col, lags=[1, 3, 6, 12], rolling_windows=[3, 6, 12]):\n",
    "    \"\"\"\n",
    "    Create lag features and rolling statistics\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Time series data\n",
    "    target_col : str\n",
    "        Target variable column name\n",
    "    lags : list\n",
    "        Lag periods to create\n",
    "    rolling_windows : list\n",
    "        Rolling window sizes for statistics\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Data with engineered features\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 3: FEATURE ENGINEERING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # 1. LAG FEATURES\n",
    "    print(f\"\\nüîß Creating lag features for {target_col}...\")\n",
    "    for lag in lags:\n",
    "        df_features[f'{target_col}_lag_{lag}'] = df_features[target_col].shift(lag)\n",
    "        print(f\"   ‚úì Created lag_{lag}\")\n",
    "    \n",
    "    # 2. ROLLING STATISTICS\n",
    "    print(f\"\\nüîß Creating rolling statistics...\")\n",
    "    for window in rolling_windows:\n",
    "        # Rolling mean\n",
    "        df_features[f'{target_col}_rolling_mean_{window}'] = \\\n",
    "            df_features[target_col].rolling(window=window).mean()\n",
    "        \n",
    "        # Rolling std\n",
    "        df_features[f'{target_col}_rolling_std_{window}'] = \\\n",
    "            df_features[target_col].rolling(window=window).std()\n",
    "        \n",
    "        # Rolling min/max\n",
    "        df_features[f'{target_col}_rolling_min_{window}'] = \\\n",
    "            df_features[target_col].rolling(window=window).min()\n",
    "        \n",
    "        df_features[f'{target_col}_rolling_max_{window}'] = \\\n",
    "            df_features[target_col].rolling(window=window).max()\n",
    "        \n",
    "        print(f\"   ‚úì Created rolling features (window={window})\")\n",
    "    \n",
    "    # 3. RATE OF CHANGE\n",
    "    print(f\"\\nüîß Creating rate of change features...\")\n",
    "    df_features[f'{target_col}_pct_change_1'] = df_features[target_col].pct_change(1)\n",
    "    df_features[f'{target_col}_pct_change_12'] = df_features[target_col].pct_change(12)\n",
    "    print(f\"   ‚úì Created percent change features\")\n",
    "    \n",
    "    # 4. MOMENTUM INDICATORS\n",
    "    print(f\"\\nüîß Creating momentum indicators...\")\n",
    "    df_features[f'{target_col}_momentum'] = \\\n",
    "        df_features[target_col] - df_features[target_col].shift(12)\n",
    "    print(f\"   ‚úì Created momentum features\")\n",
    "    \n",
    "    # 5. DATE/TIME FEATURES\n",
    "    print(f\"\\nüîß Creating temporal features...\")\n",
    "    df_features['month'] = df_features.index.month\n",
    "    df_features['quarter'] = df_features.index.quarter\n",
    "    df_features['year'] = df_features.index.year\n",
    "    print(f\"   ‚úì Created temporal features\")\n",
    "    \n",
    "    # Remove NaN created by feature engineering\n",
    "    initial_rows = len(df_features)\n",
    "    df_features = df_features.dropna()\n",
    "    final_rows = len(df_features)\n",
    "    \n",
    "    print(f\"\\nüìä Feature Engineering Summary:\")\n",
    "    print(f\"   Original features: {len(df.columns)}\")\n",
    "    print(f\"   Total features: {len(df_features.columns)}\")\n",
    "    print(f\"   New features: {len(df_features.columns) - len(df.columns)}\")\n",
    "    print(f\"   Rows after cleaning: {final_rows} (removed {initial_rows - final_rows})\")\n",
    "    \n",
    "    return df_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9e0a830-f96e-492f-82f9-4ad1d0baeb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================\n",
    "# STEP 4: BUILD FORECASTING MODELS\n",
    "# ============================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00767f09-8196-4df6-a584-a793d6924439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 1: ARIMA\n",
    "# ============================================\n",
    "\n",
    "def build_arima_model(train_data, test_data, order=(1,1,1)):\n",
    "    \"\"\"\n",
    "    Build and evaluate ARIMA model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_data : pd.Series\n",
    "        Training data\n",
    "    test_data : pd.Series\n",
    "        Testing data\n",
    "    order : tuple\n",
    "        ARIMA order (p, d, q)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Model results and predictions\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL 1: ARIMA\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nüîß Training ARIMA{order}...\")\n",
    "    \n",
    "    # Fit model\n",
    "    model = ARIMA(train_data, order=order)\n",
    "    model_fit = model.fit()\n",
    "    \n",
    "    print(f\"‚úÖ Model trained successfully!\")\n",
    "    print(f\"\\nModel Summary:\")\n",
    "    print(model_fit.summary())\n",
    "    \n",
    "    # Make predictions\n",
    "    print(f\"\\nüìä Generating predictions...\")\n",
    "    predictions = model_fit.forecast(steps=len(test_data))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(test_data, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(test_data, predictions)\n",
    "    mape = np.mean(np.abs((test_data - predictions) / test_data)) * 100\n",
    "    r2 = r2_score(test_data, predictions)\n",
    "    \n",
    "    print(f\"\\nüìà Performance Metrics:\")\n",
    "    print(f\"   RMSE: {rmse:.4f}\")\n",
    "    print(f\"   MAE: {mae:.4f}\")\n",
    "    print(f\"   MAPE: {mape:.2f}%\")\n",
    "    print(f\"   R¬≤ Score: {r2:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model_fit,\n",
    "        'predictions': predictions,\n",
    "        'metrics': {\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'MAPE': mape,\n",
    "            'R2': r2\n",
    "        }\n",
    "    }\n",
    "\n",
    "# MODEL 2: PROPHET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "856883bd-0c26-46aa-8438-da6826126911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "\n",
    "def build_prophet_model(train_data, test_data, seasonality_mode='multiplicative'):\n",
    "    \"\"\"\n",
    "    Build and evaluate Prophet model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_data : pd.Series\n",
    "        Training data with datetime index\n",
    "    test_data : pd.Series\n",
    "        Testing data with datetime index\n",
    "    seasonality_mode : str\n",
    "        'additive' or 'multiplicative'\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Model results and predictions\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL 2: PROPHET\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nüîß Training Prophet model (seasonality: {seasonality_mode})...\")\n",
    "    \n",
    "    # Prepare data for Prophet\n",
    "    train_df = pd.DataFrame({\n",
    "        'ds': train_data.index,\n",
    "        'y': train_data.values\n",
    "    })\n",
    "    \n",
    "    # Initialize and fit model\n",
    "    model = Prophet(\n",
    "        seasonality_mode=seasonality_mode,\n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=False,\n",
    "        daily_seasonality=False,\n",
    "        changepoint_prior_scale=0.05\n",
    "    )\n",
    "    \n",
    "    model.fit(train_df)\n",
    "    print(f\"‚úÖ Model trained successfully!\")\n",
    "    \n",
    "    # Create future dataframe\n",
    "    future = pd.DataFrame({'ds': test_data.index})\n",
    "    \n",
    "    # Make predictions\n",
    "    print(f\"\\nüìä Generating predictions...\")\n",
    "    forecast = model.predict(future)\n",
    "    predictions = forecast['yhat'].values\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(test_data, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(test_data, predictions)\n",
    "    mape = np.mean(np.abs((test_data - predictions) / test_data)) * 100\n",
    "    r2 = r2_score(test_data, predictions)\n",
    "    \n",
    "    print(f\"\\nüìà Performance Metrics:\")\n",
    "    print(f\"   RMSE: {rmse:.4f}\")\n",
    "    print(f\"   MAE: {mae:.4f}\")\n",
    "    print(f\"   MAPE: {mape:.2f}%\")\n",
    "    print(f\"   R¬≤ Score: {r2:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'forecast': forecast,\n",
    "        'predictions': predictions,\n",
    "        'metrics': {\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'MAPE': mape,\n",
    "            'R2': r2\n",
    "        }\n",
    "    }\n",
    "\n",
    "# MODEL 3: LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afa0bc08-a263-43a0-8dbc-3ab12de02a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "\n",
    "def prepare_lstm_data(data, look_back=12):\n",
    "    \"\"\"\n",
    "    Prepare data for LSTM model\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - look_back):\n",
    "        X.append(data[i:(i + look_back)])\n",
    "        y.append(data[i + look_back])\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4899179b-f064-4ea2-882b-5955e4200886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(train_data, test_data, look_back=12, epochs=50, batch_size=32):\n",
    "    \"\"\"\n",
    "    Build and evaluate LSTM model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_data : pd.Series\n",
    "        Training data\n",
    "    test_data : pd.Series\n",
    "        Testing data\n",
    "    look_back : int\n",
    "        Number of previous time steps to use\n",
    "    epochs : int\n",
    "        Training epochs\n",
    "    batch_size : int\n",
    "        Batch size for training\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Model results and predictions\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL 3: LSTM\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nüîß Preparing data for LSTM (look_back={look_back})...\")\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_scaled = scaler.fit_transform(train_data.values.reshape(-1, 1))\n",
    "    test_scaled = scaler.transform(test_data.values.reshape(-1, 1))\n",
    "    \n",
    "    # Prepare sequences\n",
    "    X_train, y_train = prepare_lstm_data(train_scaled.flatten(), look_back)\n",
    "    X_test, y_test = prepare_lstm_data(test_scaled.flatten(), look_back)\n",
    "    \n",
    "    # Reshape for LSTM [samples, time steps, features]\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "    \n",
    "    print(f\"   Train shape: {X_train.shape}\")\n",
    "    print(f\"   Test shape: {X_test.shape}\")\n",
    "    \n",
    "    # Build LSTM model\n",
    "    print(f\"\\nüîß Building LSTM architecture...\")\n",
    "    model = Sequential([\n",
    "        LSTM(50, return_sequences=True, input_shape=(look_back, 1)),\n",
    "        Dropout(0.2),\n",
    "        LSTM(50, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(25),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    print(f\"\\nModel Architecture:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"\\nüîß Training LSTM model...\")\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Model trained successfully!\")\n",
    "    print(f\"   Final training loss: {history.history['loss'][-1]:.6f}\")\n",
    "    print(f\"   Final validation loss: {history.history['val_loss'][-1]:.6f}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    print(f\"\\nüìä Generating predictions...\")\n",
    "    predictions_scaled = model.predict(X_test, verbose=0)\n",
    "    predictions = scaler.inverse_transform(predictions_scaled)\n",
    "    \n",
    "    # Get actual test values (accounting for look_back)\n",
    "    test_actual = test_data.values[look_back:]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(test_actual, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(test_actual, predictions)\n",
    "    mape = np.mean(np.abs((test_actual - predictions.flatten()) / test_actual)) * 100\n",
    "    r2 = r2_score(test_actual, predictions)\n",
    "    \n",
    "    print(f\"\\nüìà Performance Metrics:\")\n",
    "    print(f\"   RMSE: {rmse:.4f}\")\n",
    "    print(f\"   MAE: {mae:.4f}\")\n",
    "    print(f\"   MAPE: {mape:.2f}%\")\n",
    "    print(f\"   R¬≤ Score: {r2:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'scaler': scaler,\n",
    "        'predictions': predictions.flatten(),\n",
    "        'history': history,\n",
    "        'look_back': look_back,\n",
    "        'test_actual': test_actual,\n",
    "        'metrics': {\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'MAPE': mape,\n",
    "            'R2': r2\n",
    "        }\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "764a6338-d647-47b9-9498-1a613d345e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 5: MODEL EVALUATION & COMPARISON\n",
    "# ============================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e6abac4-b787-44cf-a652-d16d543454d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(models_dict, test_data, model_names=['ARIMA', 'Prophet', 'LSTM']):\n",
    "    \"\"\"\n",
    "    Compare performance of multiple models\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    models_dict : dict\n",
    "        Dictionary of model results\n",
    "    test_data : pd.Series\n",
    "        Test data for comparison\n",
    "    model_names : list\n",
    "        Names of models to compare\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Comparison table\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 5: MODEL COMPARISON & EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for name in model_names:\n",
    "        if name in models_dict:\n",
    "            metrics = models_dict[name]['metrics']\n",
    "            comparison_data.append({\n",
    "                'Model': name,\n",
    "                'RMSE': metrics['RMSE'],\n",
    "                'MAE': metrics['MAE'],\n",
    "                'MAPE': f\"{metrics['MAPE']:.2f}%\",\n",
    "                'R¬≤ Score': metrics['R2']\n",
    "            })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    print(\"\\nüìä Model Performance Comparison:\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Find best model\n",
    "    best_model_rmse = comparison_df.loc[comparison_df['RMSE'].idxmin(), 'Model']\n",
    "    best_model_r2 = comparison_df.loc[comparison_df['R¬≤ Score'].idxmax(), 'Model']\n",
    "    \n",
    "    print(f\"\\nüèÜ Best Models:\")\n",
    "    print(f\"   Lowest RMSE: {best_model_rmse}\")\n",
    "    print(f\"   Highest R¬≤: {best_model_r2}\")\n",
    "    \n",
    "    return comparison_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc50ec7f-f2b0-4ebe-b839-b04d6bdf4c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(test_data, models_dict, model_names=['ARIMA', 'Prophet', 'LSTM']):\n",
    "    \"\"\"\n",
    "    Plot actual vs predicted values for all models\n",
    "    \"\"\"\n",
    "    print(\"\\nüìä Generating comparison plots...\")\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Actual values\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=test_data.index[:len(test_data)],\n",
    "        y=test_data.values,\n",
    "        mode='lines',\n",
    "        name='Actual',\n",
    "        line=dict(color='black', width=3)\n",
    "    ))\n",
    "    \n",
    "    # Model predictions\n",
    "    colors = ['blue', 'red', 'green']\n",
    "    for idx, name in enumerate(model_names):\n",
    "        if name in models_dict:\n",
    "            predictions = models_dict[name]['predictions']\n",
    "            \n",
    "            # Align predictions with test data index\n",
    "            if name == 'LSTM':\n",
    "                # LSTM predictions start after look_back period\n",
    "                look_back = models_dict[name]['look_back']\n",
    "                x_vals = test_data.index[look_back:look_back+len(predictions)]\n",
    "            else:\n",
    "                x_vals = test_data.index[:len(predictions)]\n",
    "            \n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=x_vals,\n",
    "                y=predictions,\n",
    "                mode='lines',\n",
    "                name=f'{name} (MAPE: {models_dict[name][\"metrics\"][\"MAPE\"]:.2f}%)',\n",
    "                line=dict(color=colors[idx], width=2, dash='dash')\n",
    "            ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Model Predictions Comparison',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Value',\n",
    "        hovermode='x unified',\n",
    "        height=600,\n",
    "        template='plotly_white',\n",
    "        legend=dict(x=0.01, y=0.99)\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    print(\"‚úÖ Comparison plot generated!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "30375c74-5c83-43b7-a680-491a9be767e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals(test_data, models_dict, model_names=['ARIMA', 'Prophet', 'LSTM']):\n",
    "    \"\"\"\n",
    "    Plot residuals for all models\n",
    "    \"\"\"\n",
    "    print(\"\\nüìä Generating residual plots...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(len(model_names), 2, figsize=(14, 4*len(model_names)))\n",
    "    \n",
    "    for idx, name in enumerate(model_names):\n",
    "        if name in models_dict:\n",
    "            predictions = models_dict[name]['predictions']\n",
    "            \n",
    "            # Calculate residuals\n",
    "            if name == 'LSTM':\n",
    "                look_back = models_dict[name]['look_back']\n",
    "                actual = test_data.values[look_back:look_back+len(predictions)]\n",
    "            else:\n",
    "                actual = test_data.values[:len(predictions)]\n",
    "            \n",
    "            residuals = actual - predictions\n",
    "            \n",
    "            # Residual plot\n",
    "            axes[idx, 0].plot(residuals, linewidth=1.5, color='blue')\n",
    "            axes[idx, 0].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "            axes[idx, 0].set_title(f'{name} - Residuals Over Time')\n",
    "            axes[idx, 0].set_xlabel('Time')\n",
    "            axes[idx, 0].set_ylabel('Residual')\n",
    "            axes[idx, 0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Residual histogram\n",
    "            axes[idx, 1].hist(residuals, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "            axes[idx, 1].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "            axes[idx, 1].set_title(f'{name} - Residual Distribution')\n",
    "            axes[idx, 1].set_xlabel('Residual')\n",
    "            axes[idx, 1].set_ylabel('Frequency')\n",
    "            axes[idx, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Residual plots generated!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd4e41eb-d887-4c61-8205-bd7372763958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPLETE WORKFLOW FUNCTION\n",
    "# ============================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae1ca154-b1f4-4e95-b490-f723db19047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_forecasting_workflow(data, target_col, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Run complete forecasting workflow from data processing to evaluation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        Time series data with datetime index\n",
    "    target_col : str\n",
    "        Target variable to forecast\n",
    "    test_size : float\n",
    "        Proportion of data for testing\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : All results including processed data and model predictions\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üöÄ COMPLETE ECONOMIC FORECASTING WORKFLOW\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nTarget Variable: {target_col}\")\n",
    "    print(f\"Data Shape: {data.shape}\")\n",
    "    print(f\"Date Range: {data.index.min().date()} to {data.index.max().date()}\")\n",
    "    \n",
    "    # Step 1: Handle missing values\n",
    "    data_clean = handle_missing_values(data, method='hybrid')\n",
    "    \n",
    "    # Step 2: Test stationarity (keep original for modeling)\n",
    "    target_series = data_clean[target_col]\n",
    "    \n",
    "    # Step 3: Feature engineering (optional - for future use)\n",
    "    # data_features = create_features(data_clean, target_col)\n",
    "    \n",
    "    # Split data\n",
    "    split_idx = int(len(target_series) * (1 - test_size))\n",
    "    train_data = target_series[:split_idx]\n",
    "    test_data = target_series[split_idx:]\n",
    "    \n",
    "    print(f\"\\nüìä Data Split:\")\n",
    "    print(f\"   Training: {len(train_data)} samples ({train_data.index.min().date()} to {train_data.index.max().date()})\")\n",
    "    print(f\"   Testing: {len(test_data)} samples ({test_data.index.min().date()} to {test_data.index.max().date()})\")\n",
    "    \n",
    "    # Step 4: Build models\n",
    "    models_results = {}\n",
    "    \n",
    "    # ARIMA\n",
    "    try:\n",
    "        models_results['ARIMA'] = build_arima_model(train_data, test_data, order=(2,1,2))\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ARIMA failed: {e}\")\n",
    "    \n",
    "    # Prophet\n",
    "    try:\n",
    "        models_results['Prophet'] = build_prophet_model(train_data, test_data)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Prophet failed: {e}\")\n",
    "    \n",
    "    # LSTM\n",
    "    try:\n",
    "        models_results['LSTM'] = build_lstm_model(train_data, test_data, look_back=12, epochs=50)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå LSTM failed: {e}\")\n",
    "    \n",
    "    # Step 5: Compare and evaluate\n",
    "    comparison_df = compare_models(models_results, test_data)\n",
    "    \n",
    "    # Generate plots\n",
    "    plot_predictions(test_data, models_results)\n",
    "    plot_residuals(test_data, models_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ WORKFLOW COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return {\n",
    "        'data_clean': data_clean,\n",
    "        'train_data': train_data,\n",
    "        'test_data': test_data,\n",
    "        'models': models_results,\n",
    "        'comparison': comparison_df\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f6ee0a71-cd16-4694-9641-3035e7583a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXAMPLE USAGE\n",
    "# ============================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c11cf76f-0b3d-4927-a573-ef405fae169a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EXAMPLE: Running Complete Workflow\n",
      "============================================================\n",
      "\n",
      "üí° To run this workflow with your data:\n",
      "\n",
      "# Load your data\n",
      "economic_data = pd.read_csv('economic_indicators.csv', index_col=0, parse_dates=True)\n",
      "\n",
      "# Run complete workflow\n",
      "results = complete_forecasting_workflow(\n",
      "    data=economic_data,\n",
      "    target_col='Unemployment Rate',  # or 'Consumer Price Index', etc.\n",
      "    test_size=0.2\n",
      ")\n",
      "\n",
      "# Access results\n",
      "comparison_table = results['comparison']\n",
      "best_model = results['models']['Prophet']  # or 'ARIMA', 'LSTM'\n",
      "\n",
      "‚úÖ All functions defined and ready to use!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXAMPLE: Running Complete Workflow\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load your data (replace with your actual data)\n",
    "    # Assuming you have 'economic_data' from previous setup\n",
    "    \n",
    "    # For demo, let's create sample data\n",
    "    # In practice, use: economic_data = pd.read_csv('economic_indicators.csv', index_col=0, parse_dates=True)\n",
    "    \n",
    "    print(\"\\nüí° To run this workflow with your data:\")\n",
    "    print(\"\\n# Load your data\")\n",
    "    print(\"economic_data = pd.read_csv('economic_indicators.csv', index_col=0, parse_dates=True)\")\n",
    "    print(\"\\n# Run complete workflow\")\n",
    "    print(\"results = complete_forecasting_workflow(\")\n",
    "    print(\"    data=economic_data,\")\n",
    "    print(\"    target_col='Unemployment Rate',  # or 'Consumer Price Index', etc.\")\n",
    "    print(\"    test_size=0.2\")\n",
    "    print(\")\")\n",
    "    print(\"\\n# Access results\")\n",
    "    print(\"comparison_table = results['comparison']\")\n",
    "    print(\"best_model = results['models']['Prophet']  # or 'ARIMA', 'LSTM'\")\n",
    "    \n",
    "    print(\"\\n‚úÖ All functions defined and ready to use!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cdcf97-01e9-4963-8056-ac13f6163525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
